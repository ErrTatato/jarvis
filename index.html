<!DOCTYPE html>
<html lang="it">
<head>
  <meta charset="utf-8" />
  <title>Jarvis - WebSocket streaming</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    body { font-family: Segoe UI, Tahoma, sans-serif; background: linear-gradient(#0f1117,#1c1f2a); color:#c9d1d9; margin:0; height:100vh; overflow:hidden }
    header { display:flex; justify-content:space-between; padding:12px 16px; align-items:center }
    h1 { color:#58a6ff; margin:0; font-size:1.1rem }
    .controls { display:flex; gap:10px; align-items:center }
    button { background:#238636; border:none; color:white; padding:8px 12px; border-radius:8px; cursor:pointer }
    main { display:flex; gap:14px; padding:12px; height:calc(100vh - 60px); box-sizing:border-box }
    .left { flex:1; background:rgba(255,255,255,0.02); border-radius:10px; padding:12px; overflow:auto }
    .right { width:340px; background:rgba(255,255,255,0.02); border-radius:10px; padding:12px; overflow:auto }
    .log { font-family:monospace; font-size:13px; white-space:pre-wrap }
    audio { width:100%; margin-top:8px; border-radius:8px }
    #mic { width:54px; height:54px; border-radius:50%; background:#222; display:flex; align-items:center; justify-content:center }
    #mic.on { box-shadow:0 0 18px rgba(126,252,141,0.4) }
  </style>
</head>
<body>
<header>
  <h1>Jarvis (WS)</h1>
  <div class="controls">
    <div id="mic" title="microfono">üéôÔ∏è</div>
    <button id="btn-start">Avvia</button>
    <button id="btn-stop">Ferma</button>
  </div>
</header>
<main>
  <div class="left">
    <div><strong>Transcript (local)</strong></div>
    <div id="trans" class="log"></div>
    <div style="margin-top:8px"><strong>GPT partial / final</strong></div>
    <div id="gpt" class="log"></div>
    <audio id="player" autoplay></audio>
  </div>
  <div class="right">
    <div><strong>Debug</strong></div>
    <div id="debug" class="log"></div>
    <div style="margin-top:12px"><strong>Actions</strong></div>
    <div id="act" class="log"></div>
  </div>
</main>

<script>
/*
Complete client JS:
- stable WebSocket in `ws`
- VAD capture via WebAudio, resample to 16000, encode WAV PCM16
- send with ws.send(...)
- handle server messages: transcript, partial_text, audio_chunk, done, error
- queue audio chunks and play immediately without re-recording (blocked while playing)
- robust logging and safety checks
*/

const WS_URL = (location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + '/ws';
let ws = null;

const btnStart = document.getElementById('btn-start');
const btnStop = document.getElementById('btn-stop');
const transEl = document.getElementById('trans');
const gptEl = document.getElementById('gpt');
const debugEl = document.getElementById('debug');
const actEl = document.getElementById('act');
const micEl = document.getElementById('mic');
const player = document.getElementById('player');

let audioContext = null;
let sourceNode = null;
let processor = null;
let stream = null;

let recording = false;
let buffer = [];
let bufferLen = 0;
let sampleRate = 48000;
const OUT_RATE = 16000;

const SILENCE_DB = -50;     // threshold
const SILENCE_MS = 700;     // end-of-speech timeout
let voiceActive = false;
let silenceTimer = null;
let blocked = false;        // block capturing while sending / playing

// audio queue for progressive chunks
const audioQueue = [];

// utility logs
function logDebug(s){ debugEl.innerText = `[${new Date().toLocaleTimeString()}] ${s}\n` + debugEl.innerText; }
function logAct(s){ actEl.innerText = `[${new Date().toLocaleTimeString()}] ${s}\n` + actEl.innerText; }

function enableMicUI(v){ micEl.classList.toggle('on', v); }

// RMS to dB
function rmsToDb(rms){ const min=1e-8; return 20*Math.log10(Math.max(rms,min)); }

// WebSocket connect
function wsConnect(){
  if(ws && (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING)) return;
  logDebug('Connecting WebSocket...');
  ws = new WebSocket(WS_URL);

  ws.onopen = () => {
    logDebug('WS connected');
    try { ws.send(JSON.stringify({type:'start_session', session_id:'matteo'})); }
    catch(e){ logDebug('WS send start_session failed: '+e); }
  };

  ws.onmessage = (ev) => {
    try {
      const msg = JSON.parse(ev.data);
      handleServerMsg(msg);
    } catch(e) {
      logDebug('ws parse err: '+ e);
    }
  };

  ws.onclose = (ev) => {
    logDebug('WS closed code=' + (ev.code || 'n/a'));
    ws = null;
  };

  ws.onerror = (e) => { logDebug('WS error'); };
}

// handle incoming server messages
function handleServerMsg(msg){
  if(!msg || !msg.type) return;
  if(msg.type === 'info'){ logDebug('info: '+msg.message); }
  else if(msg.type === 'transcript'){ transEl.innerText = msg.text; }
  else if(msg.type === 'partial_text'){ gptEl.innerText += msg.delta; }
  else if(msg.type === 'audio_chunk'){
    if(msg.audio_b64){
      enqueueAudioChunk(msg.audio_b64);
    }
  }
  else if(msg.type === 'done'){ gptEl.innerText = msg.full; }
  else if(msg.type === 'error'){ logDebug('server error: '+msg.message); }
}

// enqueue and play audio chunks sequentially
function enqueueAudioChunk(b64){
  audioQueue.push(b64);
  if(!player.src || player.ended || player.paused){
    playNextChunk();
  } else {
    // it will be played when current ends because of ended handler
  }
}

// play next audio chunk from queue
function playNextChunk(){
  if(audioQueue.length === 0){ blocked = false; logAct('no queued audio, listening re-enabled'); return; }
  const b64 = audioQueue.shift();
  const bin = atob(b64);
  const len = bin.length;
  const arr = new Uint8Array(len);
  for(let i=0;i<len;i++) arr[i] = bin.charCodeAt(i);
  const blob = new Blob([arr.buffer], {type: 'audio/mpeg'});
  const url = URL.createObjectURL(blob);
  blocked = true;
  player.src = url;
  player.play().catch(e => { logDebug('play err: '+e); blocked = false; });
}

// when playback ends, play next or re-enable listening
player.addEventListener('ended', ()=>{
  // small delay to avoid capturing TTS
  setTimeout(()=> {
    if(audioQueue.length>0) playNextChunk();
    else { blocked = false; logAct('playback ended, listening re-enabled'); }
  }, 220);
});

// capture control
btnStart.onclick = ()=> startCapture();
btnStop.onclick = ()=> stopCapture();

async function startCapture(){
  if(recording) return;
  if(!ws) wsConnect();
  try {
    stream = await navigator.mediaDevices.getUserMedia({audio:true});
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
    sampleRate = audioContext.sampleRate || 48000;
    sourceNode = audioContext.createMediaStreamSource(stream);
    processor = audioContext.createScriptProcessor(4096,1,1);
    processor.onaudioprocess = onAudio;
    sourceNode.connect(processor);
    processor.connect(audioContext.destination);
    recording = true;
    logAct('started capture');
    enableMicUI(true);
  } catch(e) {
    logDebug('getUserMedia err: '+ e);
  }
}

function stopCapture(){
  if(!recording) return;
  try {
    processor.disconnect();
    sourceNode.disconnect();
    stream.getTracks().forEach(t => t.stop());
    audioContext.close();
  } catch(e){}
  recording = false;
  buffer = []; bufferLen = 0;
  enableMicUI(false);
  logAct('stopped capture');
}

// audio processing and VAD
function onAudio(e){
  if(!recording || blocked) return;
  const input = e.inputBuffer.getChannelData(0);
  let sum = 0;
  for(let i=0;i<input.length;i++) sum += input[i]*input[i];
  const rms = Math.sqrt(sum/input.length);
  const db = rmsToDb(rms);

  if(db > SILENCE_DB){
    // voice present
    voiceActive = true;
    lastPiecePush(input);
    if(silenceTimer){ clearTimeout(silenceTimer); silenceTimer = null; }
  } else {
    // silence
    if(voiceActive && !silenceTimer){
      silenceTimer = setTimeout(()=> {
        voiceActive = false; silenceTimer = null;
        const durationMs = (bufferLen / sampleRate) * 1000;
        if(durationMs > 60){
          sendBufferedWav();
        } else {
          buffer=[]; bufferLen=0;
        }
      }, SILENCE_MS);
    } else if(voiceActive){
      lastPiecePush(input);
    }
  }
}

function lastPiecePush(chunk){
  buffer.push(new Float32Array(chunk));
  bufferLen += chunk.length;
}

// resample+encode WAV 16-bit PCM little-endian
function encodeWav(samples, outRate){
  // merge
  let total = 0; for(let i=0;i<samples.length;i++) total += samples[i].length;
  let merged = new Float32Array(total);
  let off = 0;
  for(let i=0;i<samples.length;i++){ merged.set(samples[i], off); off += samples[i].length; }

  const ratio = sampleRate / outRate;
  const newLen = Math.round(merged.length / ratio);
  const resampled = new Float32Array(newLen);
  for(let i=0;i<newLen;i++){
    const idx = i * ratio;
    const i0 = Math.floor(idx), i1 = Math.min(Math.ceil(idx), merged.length - 1);
    const frac = idx - i0;
    resampled[i] = merged[i0] * (1 - frac) + merged[i1] * frac;
  }

  const bytesPerSample = 2;
  const dataSize = resampled.length * bytesPerSample;
  const bufferOut = new ArrayBuffer(44 + dataSize);
  const view = new DataView(bufferOut);

  function writeStr(o, s){ for(let i=0;i<s.length;i++) view.setUint8(o+i, s.charCodeAt(i)); }
  writeStr(0,'RIFF'); view.setUint32(4,36+dataSize,true); writeStr(8,'WAVE'); writeStr(12,'fmt ');
  view.setUint32(16,16,true); view.setUint16(20,1,true); view.setUint16(22,1,true);
  view.setUint32(24,outRate,true); view.setUint32(28,outRate * bytesPerSample,true);
  view.setUint16(32, bytesPerSample, true); view.setUint16(34,16,true); writeStr(36,'data');
  view.setUint32(40,dataSize,true);
  let p = 44;
  for(let i=0;i<resampled.length;i++,p+=2){
    let s = Math.max(-1, Math.min(1, resampled[i]));
    view.setInt16(p, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
  }
  return bufferOut;
}

function arrayBufferToBase64(buf){
  const bytes = new Uint8Array(buf);
  const chunk = 0x8000;
  let bin = '';
  for(let i=0;i<bytes.length;i+=chunk) bin += String.fromCharCode.apply(null, Array.from(bytes.subarray(i, i+chunk)));
  return btoa(bin);
}

async function sendBufferedWav(){
  if(!ws || ws.readyState !== WebSocket.OPEN){
    logDebug('ws not open, dropping audio');
    buffer=[]; bufferLen=0;
    return;
  }
  blocked = true;
  logAct('sending buffered audio to server');
  const wav = encodeWav(buffer, OUT_RATE);
  buffer = []; bufferLen = 0;
  const b64 = arrayBufferToBase64(wav);
  logDebug('üé§ Invio audio al server...');
  try {
    ws.send(JSON.stringify({type: "wav", wav_base64: b64}));
  } catch(e){
    logDebug('ws.send failed: ' + e);
    blocked = false;
  }
}

// Init on load
window.addEventListener('load', ()=> {
  wsConnect();
  logDebug('Client ready');
});
</script>
</body>
</html>
